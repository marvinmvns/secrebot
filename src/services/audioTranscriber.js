import fs from 'fs/promises';
import path from 'path';
import { Readable } from 'stream';
import { spawn } from 'child_process';
import { WHISPER_CPP_PATH, WHISPER_CPP_MAIN_PATH, MODEL_OBJECT } from 'nodejs-whisper/dist/constants.js';
import { nodewhisper } from 'nodejs-whisper';
import ffmpeg from 'fluent-ffmpeg';
import Utils from '../utils/index.js'; // Ajustar caminho se necess√°rio
import { CONFIG, __dirname, getDynamicConfig } from '../config/index.js'; // Ajustar caminho se necess√°rio
import JobQueue from './jobQueue.js';
import logger from '../utils/logger.js';
import WhisperAPIPool from './whisperApiPool.js';
import { getMetricsService } from './metricsService.js';

// ============ Transcritor de √Åudio ============
class AudioTranscriber {
  constructor(configService = null, llmService = null) {
    this.configService = configService;
    this.llmService = llmService;
    this.queue = new JobQueue(
      CONFIG.queues.whisperConcurrency,
      CONFIG.queues.memoryThresholdGB
    );
    this.whisperApiPool = new WhisperAPIPool(configService);
    this.realtimeSessions = new Map(); // Stores active real-time transcription sessions
    this.metricsService = getMetricsService();
  }

  startRealtimeTranscription() {
    const sessionId = `rt_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    this.realtimeSessions.set(sessionId, { 
      audioBuffer: Buffer.alloc(0), 
      lastTranscription: '',
      lastChunkTime: Date.now()
    });
    logger.info(`üé§ Real-time transcription session started: ${sessionId}`);
    return sessionId;
  }

  async processRealtimeChunk(sessionId, audioChunk, isLastChunk = false) {
    const session = this.realtimeSessions.get(sessionId);
    if (!session) {
      throw new Error(`Real-time session ${sessionId} not found.`);
    }

    if (audioChunk.length === 0) {
      logger.warn(`‚ö†Ô∏è Received empty audio chunk for session ${sessionId}. Skipping processing.`);
      return session.lastTranscription; // Return last known transcription
    }

    session.audioBuffer = Buffer.concat([session.audioBuffer, audioChunk]);
    session.lastChunkTime = Date.now();

    let currentTranscription = '';

    if (isLastChunk) {
      logger.info(`üé§ Finalizing real-time transcription for session ${sessionId}. Total buffer size: ${session.audioBuffer.length} bytes.`);
      currentTranscription = await this.transcribe(session.audioBuffer, 'ogg'); // Assuming ogg for now
      this.realtimeSessions.delete(sessionId);
      logger.info(`üé§ Real-time transcription session ${sessionId} ended.`);
    } else {
      // For real-time feedback, transcribe the entire accumulated buffer
      logger.debug(`üé§ Processing real-time chunk for session ${sessionId}. Accumulated buffer size: ${session.audioBuffer.length} bytes.`);
      try {
        currentTranscription = await this.transcribe(session.audioBuffer, 'ogg'); 
      } catch (error) {
        logger.warn(`‚ö†Ô∏è Error transcribing real-time chunk for session ${sessionId}: ${error.message}`);
        currentTranscription = session.lastTranscription; 
      }
    }
    session.lastTranscription = currentTranscription;
    return currentTranscription;
  }

  // Existing methods follow...

  // ... (rest of the class methods) ...

  async getEffectiveWhisperOptions() {
    let mongoConfig = null;
    if (this.configService) {
      try {
        mongoConfig = await this.configService.getConfig();
      } catch (error) {
        logger.warn('‚ö†Ô∏è Erro ao obter configura√ß√£o do MongoDB para whisperOptions, usando configura√ß√£o padr√£o:', error.message);
      }
    }
    
    if (mongoConfig?.whisperApi?.whisperOptions) {
      return { ...CONFIG.whisperApi.whisperOptions, ...mongoConfig.whisperApi.whisperOptions };
    }
    
    return CONFIG.whisperApi.whisperOptions;
  }

  async transcribeWithAutoDownload(filePath, modelName = CONFIG.audio.model) {
    try {
      logger.debug(`üîÑ Iniciando transcri√ß√£o com auto-download do modelo: ${modelName}`);
      
      // Get effective whisperOptions (merged from config and MongoDB)
      const whisperOptions = await this.getEffectiveWhisperOptions();
      
      const options = {
        modelName: modelName,
        autoDownloadModelName: modelName,
        verbose: true,
        removeWavFileAfterTranscription: false,
        withCuda: false,
        whisperOptions: { 
          ...whisperOptions,
          language: CONFIG.audio.language 
        },
        logger: logger
      };

      logger.debug('üöÄ Executando nodejs-whisper com auto-download...');
      const transcription = await nodewhisper(filePath, options);
      
      logger.success(`‚úÖ Transcri√ß√£o conclu√≠da com auto-download. Modelo: ${modelName}`);
      return transcription.trim();
      
    } catch (error) {
      logger.error(`‚ùå Erro na transcri√ß√£o com auto-download:`, error);
      throw new Error(`Falha na transcri√ß√£o com auto-download: ${error.message}`);
    }
  }

  async runWhisper(filePath, options) {
    const execPath = path.join(WHISPER_CPP_PATH, WHISPER_CPP_MAIN_PATH);
    const modelFile = MODEL_OBJECT[options.modelName];
    const modelPath = path.join(WHISPER_CPP_PATH, 'models', modelFile);
    
    // Build args with whisperOptions
    const args = ['-m', modelPath, '-f', filePath, '-l', options.whisperOptions.language];
    
    // Add output format options
    if (options.whisperOptions.outputInText) args.push('-otxt');
    if (options.whisperOptions.outputInSrt) args.push('-osrt');
    if (options.whisperOptions.outputInVtt) args.push('-ovtt');
    if (options.whisperOptions.outputInLrc) args.push('-olrc');
    if (options.whisperOptions.outputInWords) args.push('-owts');
    if (options.whisperOptions.outputInCsv) args.push('-ocsv');
    if (options.whisperOptions.outputInJson) args.push('-oj');
    if (options.whisperOptions.outputInJsonFull) args.push('-of');
    
    // Add other options
    if (options.whisperOptions.translateToEnglish) args.push('--translate');
    if (options.whisperOptions.splitOnWord) args.push('--split-on-word');
    if (options.whisperOptions.timestamps_length) {
      args.push('--length');
      args.push(String(options.whisperOptions.timestamps_length));
    }

    // Log detalhado para debug
    logger.debug('üîß Whisper Debug Info:', {
      execPath,
      modelFile,
      modelPath,
      filePath,
      args: args.join(' '),
      cwd: WHISPER_CPP_PATH,
      timeout: CONFIG.audio.timeoutMs
    });

    // Verificar se o execut√°vel existe
    try {
      await fs.access(execPath, fs.constants.F_OK | fs.constants.X_OK);
      logger.debug(`‚úÖ Execut√°vel Whisper encontrado: ${execPath}`);
    } catch (error) {
      logger.error(`‚ùå Execut√°vel Whisper n√£o encontrado ou n√£o execut√°vel: ${execPath}`);
      throw new Error(`Whisper executable not found or not executable: ${execPath}`);
    }

    return new Promise((resolve, reject) => {
      logger.debug(`üöÄ Iniciando processo Whisper: ${execPath} ${args.join(' ')}`);
      
      const proc = spawn(execPath, args, { 
        cwd: WHISPER_CPP_PATH,
        stdio: ['pipe', 'pipe', 'pipe']
      });
      
      let stdout = '';
      let stderr = '';
      
      const timer = setTimeout(() => {
        logger.error('‚è∞ Whisper process timeout ap√≥s', CONFIG.audio.timeoutMs, 'ms');
        proc.kill('SIGKILL');
        reject(new Error(`Whisper process timed out after ${CONFIG.audio.timeoutMs}ms`));
      }, CONFIG.audio.timeoutMs);

      proc.stdout.on('data', d => {
        const data = d.toString();
        stdout += data;
        if (options.verbose) {
          logger.debug('üì§ Whisper STDOUT:', data.trim());
        }
      });

      proc.stderr.on('data', d => {
        const data = d.toString();
        stderr += data;
        logger.debug('üì• Whisper STDERR:', data.trim());
      });

      proc.on('error', err => {
        clearTimeout(timer);
        logger.error('‚ùå Whisper process error:', err);
        reject(err);
      });

      proc.on('close', code => {
        clearTimeout(timer);
        logger.debug(`üèÅ Whisper process finished with code: ${code}`);
        
        if (stdout) logger.debug('üì§ Final STDOUT:', stdout.trim());
        if (stderr) logger.debug('üì• Final STDERR:', stderr.trim());
        
        if (code === 0) {
          logger.debug('‚úÖ Whisper process completed successfully');
          resolve();
        } else {
          const errorMsg = stderr || `Whisper exited with code ${code}`;
          logger.error('‚ùå Whisper process failed:', errorMsg);
          reject(new Error(errorMsg));
        }
      });
    });
  }

  async getEffectiveConfig() {
    let mongoConfig = null;
    if (this.configService) {
      try {
        mongoConfig = await this.configService.getConfig();
      } catch (error) {
        logger.warn('‚ö†Ô∏è Erro ao obter configura√ß√£o do MongoDB, usando configura√ß√£o padr√£o:', error.message);
      }
    }
    return getDynamicConfig(mongoConfig);
  }

  async transcribe(audioBuffer, inputFormat = 'ogg', userId = 'unknown') {
    return this.queue.add(async () => {
      const startTime = Date.now();
      const audioSize = audioBuffer.length;
      
      logger.service('üé§ Iniciando transcri√ß√£o de √°udio...');
      
      // Obter configura√ß√£o efetiva (MongoDB tem prioridade)
      const effectiveConfig = await this.getEffectiveConfig();
      
      logger.verbose(`üìä Audio buffer details:`, {
        size: audioBuffer.length,
        format: inputFormat,
        sizeInMB: (audioBuffer.length / 1024 / 1024).toFixed(2),
        timestamp: new Date().toISOString(),
        mode: effectiveConfig.whisperApi.mode,
        enabled: effectiveConfig.whisperApi.enabled
      });

      try {
        let result;
        let mode, endpoint;

        // Verifica se deve usar API ou modo local usando configura√ß√£o efetiva
        if (effectiveConfig.whisperApi.mode === 'api' && effectiveConfig.whisperApi.enabled && await this.whisperApiPool.isEnabled() && this.whisperApiPool.hasHealthyEndpoints()) {
          logger.info('üåê Usando modo API para transcri√ß√£o (configura√ß√£o do MongoDB)');
          mode = 'api';
          endpoint = 'api_pool';
          result = await this.transcribeViaAPI(audioBuffer, inputFormat);
        } else {
          logger.info('üè† Usando modo local para transcri√ß√£o');
          mode = 'local';
          endpoint = 'local';
          result = await this.transcribeLocally(audioBuffer, inputFormat);
        }

        // Record metrics for successful transcription
        const duration = (Date.now() - startTime) / 1000;
        
        if (this.metricsService.enabled) {
          this.metricsService.recordWhisperRequest(
            userId,
            mode,
            endpoint,
            'success',
            duration,
            audioSize
          );
        }

        return result;
      } catch (error) {
        // Record metrics for failed transcription
        const duration = (Date.now() - startTime) / 1000;
        const mode = effectiveConfig.whisperApi.mode === 'api' ? 'api' : 'local';
        const endpoint = mode === 'api' ? 'api_pool' : 'local';
        
        if (this.metricsService.enabled) {
          this.metricsService.recordWhisperRequest(
            userId,
            mode,
            endpoint,
            'error',
            duration,
            audioSize
          );
          this.metricsService.recordError('whisper_transcription_error', 'whisper_service', userId);
        }

        throw error;
      }
    });
  }

  async transcribeWithEndpointInfo(audioBuffer, inputFormat = 'ogg') {
    return this.queue.add(async () => {
      logger.service('üé§ Iniciando transcri√ß√£o de √°udio com informa√ß√µes de endpoint...');
      
      // Obter configura√ß√£o efetiva (MongoDB tem prioridade)
      const effectiveConfig = await this.getEffectiveConfig();
      
      logger.verbose(`üìä Audio buffer details:`, {
        size: audioBuffer.length,
        format: inputFormat,
        sizeInMB: (audioBuffer.length / 1024 / 1024).toFixed(2),
        timestamp: new Date().toISOString(),
        mode: effectiveConfig.whisperApi.mode,
        enabled: effectiveConfig.whisperApi.enabled
      });

      // Verifica se deve usar API ou modo local usando configura√ß√£o efetiva
      if (effectiveConfig.whisperApi.mode === 'api' && effectiveConfig.whisperApi.enabled && await this.whisperApiPool.isEnabled() && this.whisperApiPool.hasHealthyEndpoints()) {
        logger.info('üåê Usando modo API para transcri√ß√£o (configura√ß√£o do MongoDB)');
        return await this.transcribeViaAPIWithEndpointInfo(audioBuffer, inputFormat);
      }
      
      logger.info('üè† Usando modo local para transcri√ß√£o');
      const transcription = await this.transcribeLocally(audioBuffer, inputFormat);
      return {
        transcription,
        endpoint: {
          type: 'local',
          url: 'Whisper Local (CPU)',
          mode: 'local'
        }
      };
    });
  }

  /**
   * Remove timestamp lines from Whisper transcription output
   * @param {string} text - The text containing timestamps
   * @returns {string} The cleaned text without timestamp lines
   */
  removeTimestampsFromText(text) {
    if (!text || typeof text !== 'string') {
      return text;
    }
    
    // Pattern to match timestamp lines: [HH:MM:SS.mmm] -> [HH:MM:SS.mmm]
    const timestampPattern = /^\s*\[\d{2}:\d{2}:\d{2}\.\d{3,4}\]\s*->\s*\[\d{2}:\d{2}:\d{2}\.\d{3,4}\]\s*$/gm;
    
    // Split text into lines, filter out timestamp lines, and rejoin
    const lines = text.split('\n');
    const cleanedLines = lines.filter(line => !timestampPattern.test(line));
    
    return cleanedLines.join('\n').trim();
  }

  async transcribeViaAPI(audioBuffer, inputFormat = 'ogg') {
    try {
      const timestamp = Date.now();
      const filename = `audio_${timestamp}.${inputFormat}`;
      
      logger.debug(`üåê Iniciando transcri√ß√£o via API para arquivo: ${filename}`);
      
      // Get effective whisperOptions for API transcription
      const whisperOptions = await this.getEffectiveWhisperOptions();
      
      const options = {
        language: CONFIG.audio.language,
        cleanup: true,
        ...whisperOptions
      };

      let transcription = await this.whisperApiPool.transcribe(audioBuffer, filename, options);
      
      // Remove timestamps if the option is enabled
      if (options.removeTimestamps) {
        logger.debug('üßπ Removendo timestamps da transcri√ß√£o...');
        transcription = this.removeTimestampsFromText(transcription);
      }
      
      logger.success('‚úÖ Transcri√ß√£o via API conclu√≠da com sucesso');
      return transcription;
      
    } catch (error) {
      logger.error('‚ùå Erro na transcri√ß√£o via API:', error);
      
      // Always try local fallback when API fails, regardless of mode
      logger.warn('üîÑ Todos os endpoints API falharam, tentando modo local como fallback...');
      try {
        return await this.transcribeLocally(audioBuffer, inputFormat);
      } catch (localError) {
        logger.error('‚ùå Fallback local tamb√©m falhou:', localError.message);
        throw new Error(`API transcription failed: ${error.message}. Local fallback failed: ${localError.message}`);
      }
    }
  }

  async transcribeViaAPIWithEndpointInfo(audioBuffer, inputFormat = 'ogg') {
    try {
      const timestamp = Date.now();
      const filename = `audio_${timestamp}.${inputFormat}`;
      
      logger.debug(`üåê Iniciando transcri√ß√£o via API para arquivo: ${filename}`);
      
      // Get effective whisperOptions for API transcription
      const whisperOptions = await this.getEffectiveWhisperOptions();
      
      const options = {
        language: CONFIG.audio.language,
        cleanup: true,
        ...whisperOptions
      };

      // Use the load balancing method that returns endpoint info
      const result = await this.whisperApiPool.transcribeWithLoadBalancing(audioBuffer, filename, options);
      
      // Handle different possible response structures
      let transcription;
      if (result.result && result.result.result && result.result.result.text) {
        // New API structure: result.result.result.text
        transcription = result.result.result.text;
      } else if (result.result && result.result.text) {
        // Previous structure: result.result.text
        transcription = result.result.text;
      } else if (result.result && typeof result.result === 'string') {
        // String result: result.result
        transcription = result.result;
      } else if (result.text) {
        // Direct text: result.text
        transcription = result.text;
      } else if (typeof result.result === 'object' && result.result.transcription) {
        // Alternative structure: result.result.transcription
        transcription = result.result.transcription;
      } else {
        logger.error('‚ùå Estrutura de resposta inesperada:', result);
        throw new Error('Estrutura de resposta da API inesperada');
      }
      
      // Remove timestamps if the option is enabled
      if (options.removeTimestamps) {
        logger.debug('üßπ Removendo timestamps da transcri√ß√£o...');
        transcription = this.removeTimestampsFromText(transcription);
      }
      
      logger.success('‚úÖ Transcri√ß√£o via API conclu√≠da com sucesso');
      
      return {
        transcription,
        endpoint: {
          type: 'api',
          url: result.endpoint,
          mode: 'api',
          duration: result.duration
        }
      };
      
    } catch (error) {
      logger.error('‚ùå Erro na transcri√ß√£o via API:', error);
      
      // Always try local fallback when API fails, regardless of mode
      logger.warn('üîÑ Todos os endpoints API falharam, tentando modo local como fallback...');
      try {
        const transcription = await this.transcribeLocally(audioBuffer, inputFormat);
        return {
          transcription,
          endpoint: {
            type: 'local',
            url: 'Whisper Local (CPU) - Fallback ap√≥s falha na API',
            mode: 'local-fallback'
          }
        };
      } catch (localError) {
        logger.error('‚ùå Fallback local tamb√©m falhou:', localError.message);
        throw new Error(`API transcription failed: ${error.message}. Local fallback failed: ${localError.message}`);
      }
    }
  }

  async transcribeLocally(audioBuffer, inputFormat = 'ogg') {
    const timestamp = Date.now();
    const tempOutputPath = path.join(__dirname, `audio_${timestamp}.wav`);
    logger.verbose(`üìÅ Arquivo tempor√°rio: ${tempOutputPath}`);

    try {
      // Verifica se o modelo est√° dispon√≠vel
      const modelFile = MODEL_OBJECT[CONFIG.audio.model];
      const modelPath = path.join(WHISPER_CPP_PATH, 'models', modelFile);
      logger.verbose(`üîç Verificando modelo Whisper:`, {
        model: CONFIG.audio.model,
        modelFile,
        modelPath,
        language: CONFIG.audio.language
      });
      
      try {
        await fs.access(modelPath);
        logger.verbose(`‚úÖ Modelo Whisper encontrado e acess√≠vel: ${modelPath}`);
      } catch (error) {
        logger.warn(`‚ö†Ô∏è Modelo Whisper n√£o encontrado: ${modelPath}`);
        logger.info(`üîÑ Tentando baixar automaticamente o modelo '${CONFIG.audio.model}'...`);
        
        try {
          const transcription = await this.transcribeWithAutoDownload(tempOutputPath, CONFIG.audio.model);
          logger.success(`‚úÖ Transcri√ß√£o conclu√≠da com download autom√°tico do modelo`);
          
          // Limpa arquivos tempor√°rios
          await Utils.cleanupFile(tempOutputPath);
          
          return transcription;
        } catch (autoDownloadError) {
          logger.error(`‚ùå Falha no download autom√°tico do modelo:`, autoDownloadError);
          throw new Error(`Modelo Whisper '${CONFIG.audio.model}' n√£o encontrado em ${modelPath} e falha no download autom√°tico: ${autoDownloadError.message}`);
        }
      }

      logger.verbose(`üîÑ Iniciando convers√£o de √°udio com FFMPEG:`, {
        inputFormat,
        outputFormat: 'wav',
        sampleRate: CONFIG.audio.sampleRate,
        outputPath: tempOutputPath
      });

      await new Promise((resolve, reject) => {
        const inputStream = Readable.from(audioBuffer);
        const ffmpegCommand = ffmpeg(inputStream)
          .inputFormat(inputFormat)
          .outputOptions(`-ar ${CONFIG.audio.sampleRate}`)
          .toFormat('wav')
          .on('start', (commandLine) => {
            logger.verbose(`üöÄ FFMPEG comando: ${commandLine}`);
          })
          .on('progress', (progress) => {
            logger.verbose(`‚è≥ FFMPEG progresso: ${progress.percent}%`);
          })
          .on('error', (err) => {
            logger.error('‚ùå Erro no FFMPEG:', err);
            reject(err);
          })
          .on('end', () => {
            logger.verbose('‚úÖ Convers√£o FFMPEG conclu√≠da');
            resolve();
          })
          .save(tempOutputPath);
      });
        
      const options = {
        modelName: CONFIG.audio.model,
        autoDownloadModelName: CONFIG.audio.model,
        verbose: true,
        removeWavFileAfterTranscription: false,
        withCuda: false,
        whisperOptions: { 
          outputInText: true, 
          language: CONFIG.audio.language 
        }
      };
      
      logger.verbose(`üéôÔ∏è Iniciando transcri√ß√£o Whisper:`, {
        model: options.modelName,
        language: options.whisperOptions.language,
        inputFile: tempOutputPath,
        timeout: CONFIG.audio.timeoutMs
      });
      
      // Executa o Whisper com controle de timeout
      const whisperStartTime = Date.now();
      await this.runWhisper(tempOutputPath, options);
      const whisperEndTime = Date.now();
      
      logger.verbose(`‚è±Ô∏è Transcri√ß√£o Whisper conclu√≠da em ${whisperEndTime - whisperStartTime}ms`);
      
      const transcriptionPath = `${tempOutputPath}.txt`;
      logger.verbose(`üìÑ Lendo transcri√ß√£o de: ${transcriptionPath}`);
      
      const transcription = await fs.readFile(transcriptionPath, 'utf8');
      logger.verbose(`üìù Transcri√ß√£o obtida:`, {
        length: transcription.length,
        preview: transcription.substring(0, 100) + (transcription.length > 100 ? '...' : ''),
        wordCount: transcription.split(' ').length
      });
      
      // Usa o m√©todo est√°tico de Utils para limpar arquivos
      logger.verbose(`üßπ Limpando arquivos tempor√°rios...`);
      await Utils.cleanupFile(tempOutputPath);
      await Utils.cleanupFile(transcriptionPath);
      logger.verbose(`‚úÖ Arquivos tempor√°rios removidos`);
      
      const finalTranscription = transcription.trim();
      logger.success(`‚úÖ Transcri√ß√£o local conclu√≠da. Resultado: ${finalTranscription.length} caracteres, ${finalTranscription.split(' ').length} palavras`);
      return finalTranscription;
      
    } catch (err) {
      logger.error('‚ùå Erro na transcri√ß√£o local de √°udio:', err);
      // Tenta limpar o arquivo tempor√°rio mesmo em caso de erro
      await Utils.cleanupFile(tempOutputPath);
      throw err;
    }
  }

  async transcribeAndSummarize(audioBuffer, inputFormat = 'ogg', userId = 'unknown') {
    const startTime = Date.now();
    const audioSize = audioBuffer.length;
    
    try {
      logger.service('üé§ Iniciando transcri√ß√£o e resumo de √°udio...');
      
      const transcription = await this.transcribe(audioBuffer, inputFormat, userId);
      
      logger.service('üß† Gerando resumo com LLM...');
      const summaryPrompt = `Analise a seguinte transcri√ß√£o de √°udio e crie um resumo estruturado e claro:

TRANSCRI√á√ÉO:
"${transcription}"

O resumo deve incluir os principais pontos discutidos, eventos importantes e qualquer informa√ß√£o relevante mencionada. Evite detalhes excessivos e mantenha o foco nos aspectos mais significativos da conversa. O resumo deve ser escrito em portugu√™s e ser facilmente compreens√≠vel.
Mantenha o resumo conciso mas informativo, destacando os pontos mais importantes do √°udio.`;

      let summary;
      if (this.llmService) {
        summary = await this.llmService.generateText(summaryPrompt);
      } else {
        // Fallback to direct Ollama call if LLMService is not available
        const { Ollama } = await import('ollama');
        const ollamaClient = new Ollama({ host: CONFIG.llm.host });
        const response = await ollamaClient.generate({
          model: CONFIG.llm.model,
          prompt: summaryPrompt,
          stream: false
        });
        summary = response.response;
      }

      logger.success('‚úÖ Transcri√ß√£o e resumo conclu√≠dos.');
      
      const summaryText = typeof summary === 'string' ? summary : summary.response;
      
      // Record metrics for successful transcription and summarization
      const duration = (Date.now() - startTime) / 1000;
      const effectiveConfig = await this.getEffectiveConfig();
      const mode = effectiveConfig.whisperApi.mode === 'api' ? 'api' : 'local';
      const endpoint = mode === 'api' ? 'api_pool' : 'local';
      
      if (this.metricsService.enabled) {
        this.metricsService.recordWhisperRequest(
          userId,
          mode,
          endpoint,
          'success',
          duration,
          audioSize
        );
      }
      
      return {
        transcription: transcription,
        summary: summaryText,
        combined: `üé§ **TRANSCRI√á√ÉO COMPLETA:**\n\n${transcription}\n\n---\n\n${summaryText}`
      };
      
    } catch (err) {
      // Record metrics for failed transcription and summarization
      const duration = (Date.now() - startTime) / 1000;
      const effectiveConfig = await this.getEffectiveConfig();
      const mode = effectiveConfig.whisperApi.mode === 'api' ? 'api' : 'local';
      const endpoint = mode === 'api' ? 'api_pool' : 'local';
      
      if (this.metricsService.enabled) {
        this.metricsService.recordWhisperRequest(
          userId,
          mode,
          endpoint,
          'error',
          duration,
          audioSize
        );
        this.metricsService.recordError('whisper_transcribe_summarize_error', 'whisper_service', userId);
      }
      
      logger.error('‚ùå Erro na transcri√ß√£o e resumo de √°udio:', err);
      throw err;
    }
  }

  async getWhisperApiStatus() {
    const effectiveConfig = await this.getEffectiveConfig();
    const isEnabled = await this.whisperApiPool.isEnabled();

    if (!isEnabled) {
      return {
        available: false,
        mode: effectiveConfig.whisperApi.mode,
        message: 'Whisper API n√£o est√° habilitado na configura√ß√£o.',
        clients: []
      };
    }

    try {
      const poolStatus = await this.whisperApiPool.getPoolStatus();
      
      // O frontend espera uma propriedade `clients`
      return {
        available: true,
        mode: effectiveConfig.whisperApi.mode,
        clients: poolStatus.endpoints || [], // Mapeia endpoints para clients
        healthy: poolStatus.healthyEndpoints > 0,
        stats: {
          total: poolStatus.totalEndpoints,
          healthy: poolStatus.healthyEndpoints,
          unhealthy: poolStatus.totalEndpoints - poolStatus.healthyEndpoints
        }
      };
    } catch (error) {
      logger.error('Erro ao obter status do Whisper API Pool:', error);
      return {
        available: true, // Still enabled, but in error state
        mode: effectiveConfig.whisperApi.mode,
        error: error.message,
        clients: [],
        healthy: false
      };
    }
  }

  async getMode() {
    const effectiveConfig = await this.getEffectiveConfig();
    if (effectiveConfig.whisperApi.mode === 'api' && await this.whisperApiPool.isEnabled() && this.whisperApiPool.hasHealthyEndpoints()) {
      return 'api';
    }
    return 'local';
  }

  async onConfigurationChanged() {
    logger.info('üîÑ Configura√ß√£o alterada, reinicializando WhisperAPIPool...');
    await this.whisperApiPool.reinitialize();
  }

  destroy() {
    logger.info('üóëÔ∏è Destruindo AudioTranscriber...');
    this.whisperApiPool.destroy();
  }
}

export default AudioTranscriber;
