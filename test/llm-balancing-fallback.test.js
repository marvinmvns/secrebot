#!/usr/bin/env node

/**
 * Test script para verificar o balanceamento de LLM e fallback para Ollama local
 */

import LLMService from '../src/services/llmService.js';
import test from 'node:test';
import assert from 'node:assert';
import { CONFIG } from '../src/config/index.js';

test('LLMService - Balanceamento e Fallback', async () => {
  console.log('ðŸ§ª Testando balanceamento LLM e fallback...\n');
  
  try {
    // Criar instÃ¢ncia do LLMService
    const llmService = new LLMService();
    
    // 1. Verificar se o shouldUseApiPool funciona
    console.log('1. Testando detecÃ§Ã£o de API Pool...');
    const shouldUsePool = await llmService.shouldUseApiPool();
    console.log(`   ðŸ“Š Should use API Pool: ${shouldUsePool}`);
    
    // 2. Testar o mÃ©todo generateText (usado pelo crypto)
    console.log('2. Testando generateText...');
    const testPrompt = 'Responda apenas "TESTE OK"';
    
    try {
      const response = await llmService.generateText(testPrompt);
      console.log(`   âœ… generateText funcionou: ${response.substring(0, 50)}...`);
      assert(typeof response === 'string', 'Response deve ser string');
      assert(response.length > 0, 'Response nÃ£o deve estar vazio');
    } catch (error) {
      console.log(`   âš ï¸ generateText falhou, mas pode ser esperado sem Ollama: ${error.message}`);
    }
    
    // 3. Testar o mÃ©todo generateResponse (usado pelo Telegram)
    console.log('3. Testando generateResponse...');
    try {
      const response = await llmService.generateResponse('Diga "OK"');
      console.log(`   âœ… generateResponse funcionou: ${response.substring(0, 50)}...`);
      assert(typeof response === 'string', 'Response deve ser string');
    } catch (error) {
      console.log(`   âš ï¸ generateResponse falhou, mas pode ser esperado sem Ollama: ${error.message}`);
    }
    
    // 4. Verificar status do pool de APIs Ollama
    console.log('4. Testando status do OllamaAPI Pool...');
    const poolStatus = await llmService.getOllamaApiStatus();
    console.log(`   ðŸ“Š Pool habilitado: ${poolStatus.enabled}`);
    console.log(`   ðŸ“Š Endpoints totais: ${poolStatus.totalEndpoints}`);
    console.log(`   ðŸ“Š Endpoints saudÃ¡veis: ${poolStatus.healthyEndpoints}`);
    console.log(`   ðŸ“Š Modo: ${poolStatus.mode}`);
    console.log(`   ðŸ“Š EstratÃ©gia: ${poolStatus.strategy}`);
    
    if (poolStatus.enabled && poolStatus.healthyEndpoints > 0) {
      console.log('   âœ… Pool estÃ¡ ativo e funcionando');
    } else {
      console.log('   âš ï¸ Pool nÃ£o estÃ¡ ativo, usando Ollama local');
    }
    
    // 5. Testar listModels
    console.log('5. Testando listModels...');
    try {
      const models = await llmService.listModelsFromAllEndpoints();
      console.log(`   ðŸ“Š Modelos encontrados: ${models.totalModels}`);
      console.log(`   ðŸ“Š Endpoints consultados: ${models.endpoints.length}`);
      
      if (models.uniqueModels.length > 0) {
        console.log(`   ðŸ“‹ Primeiros modelos: ${models.uniqueModels.slice(0, 3).join(', ')}`);
      }
      
      assert(models.endpoints.length >= 1, 'Deve ter pelo menos 1 endpoint (local)');
    } catch (error) {
      console.log(`   âš ï¸ listModels falhou: ${error.message}`);
    }
    
    // 6. Testar chat com contexto (usado pelo WhatsApp)
    console.log('6. Testando chat com contexto...');
    try {
      const contactId = 'test_user';
      const response = await llmService.getAssistantResponse(contactId, 'Diga apenas "CHAT OK"');
      console.log(`   âœ… Chat com contexto funcionou: ${response.substring(0, 50)}...`);
      
      // Limpar contexto de teste
      await llmService.clearContext(contactId, 'assistant');
    } catch (error) {
      console.log(`   âš ï¸ Chat com contexto falhou: ${error.message}`);
    }
    
    console.log('\nâœ… Teste de balanceamento LLM concluÃ­do');
    
  } catch (error) {
    console.error('âŒ Erro no teste de balanceamento LLM:', error);
    throw error;
  }
});

test('LLMService - VerificaÃ§Ã£o de ConfiguraÃ§Ã£o', async () => {
  console.log('ðŸ§ª Verificando configuraÃ§Ã£o LLM...\n');
  
  // Verificar configuraÃ§Ãµes
  console.log(`ðŸ“Š Host Ollama: ${CONFIG.llm.host}`);
  console.log(`ðŸ“Š Modelo padrÃ£o: ${CONFIG.llm.model}`);
  console.log(`ðŸ“Š Modelo de imagem: ${CONFIG.llm.imageModel || 'llava (padrÃ£o)'}`);
  
  // Criar serviÃ§o e verificar effective config
  const llmService = new LLMService();
  const effectiveConfig = await llmService.getEffectiveConfig();
  
  console.log(`ðŸ“Š OllamaAPI habilitado: ${effectiveConfig.ollamaApi.enabled}`);
  console.log(`ðŸ“Š Modo OllamaAPI: ${effectiveConfig.ollamaApi.mode}`);
  
  if (effectiveConfig.ollamaApi.enabled && effectiveConfig.ollamaApi.endpoints) {
    console.log(`ðŸ“Š Endpoints configurados: ${effectiveConfig.ollamaApi.endpoints.length}`);
  }
  
  console.log('\nâœ… VerificaÃ§Ã£o de configuraÃ§Ã£o concluÃ­da');
});