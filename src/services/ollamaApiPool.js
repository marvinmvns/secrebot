import OllamaAPIClient from './ollamaApiClient.js';
import RKLlamaAPIClient from './rkllamaApiClient.js';
import logger from '../utils/logger.js';
import { CONFIG, getDynamicConfig } from '../config/index.js';

class OllamaAPIPool {
  constructor(configService = null) {
    this.configService = configService;
    this.clients = [];
    this.currentIndex = 0;
    this.lastHealthCheck = 0;
    this.healthCheckInterval = null;
    this.requestCount = 0; // Para tracking de balanceamento
    this.initialize();
  }

  async getEffectiveConfig() {
    let mongoConfig = null;
    if (this.configService) {
      try {
        mongoConfig = await this.configService.getConfig();
      } catch (error) {
        logger.warn('‚ö†Ô∏è Erro ao obter configura√ß√£o do MongoDB para OllamaAPI, usando configura√ß√£o padr√£o:', error.message);
      }
    }
    return getDynamicConfig(mongoConfig);
  }

  // Helper function to detect RKLLama endpoints (by configured type)
  isRKLlamaEndpoint(endpoint) {
    // Check configured type first, fallback to port detection for backward compatibility
    if (endpoint.type) {
      return endpoint.type === 'rkllama';
    }
    
    // Fallback: detect by port for existing configurations
    try {
      const parsedUrl = new URL(endpoint.url || endpoint);
      return parsedUrl.port === '8080';
    } catch (error) {
      logger.warn(`‚ö†Ô∏è URL inv√°lida para detec√ß√£o RKLLama: ${endpoint.url || endpoint}`);
      return false;
    }
  }

  isChatGPTEndpoint(endpoint) {
    // Check configured type first
    if (endpoint.type) {
      return endpoint.type === 'chatgpt';
    }
    
    // Fallback: detect by URL for existing configurations
    try {
      const parsedUrl = new URL(endpoint.url || endpoint);
      return parsedUrl.hostname === 'api.openai.com';
    } catch (error) {
      logger.warn(`‚ö†Ô∏è URL inv√°lida para detec√ß√£o ChatGPT: ${endpoint.url || endpoint}`);
      return false;
    }
  }

  getEndpointType(endpoint) {
    if (this.isChatGPTEndpoint(endpoint)) return 'chatgpt';
    if (this.isRKLlamaEndpoint(endpoint)) return 'rkllama';
    return 'ollama';
  }

  // Create appropriate client based on endpoint type
  async createClient(endpoint) {
    const endpointType = this.getEndpointType(endpoint);
    
    logger.debug(`üîß Criando cliente para endpoint:`, {
      url: endpoint.url,
      type: endpoint.type,
      enabled: endpoint.enabled,
      priority: endpoint.priority,
      detectedType: endpointType
    });
    
    try {
      if (endpointType === 'chatgpt') {
        logger.info(`üöÄ Criando cliente ChatGPT para: ${endpoint.url}`);
        const chatGPTModule = await import('./chatgptApiClient.js');
        const { ChatGPTAPIClient } = chatGPTModule;
        
        const options = {
          useStreaming: endpoint.useStreaming !== false, // Default true
          model: endpoint.model || 'gpt-4'
        };
        
        return new ChatGPTAPIClient(endpoint.url, endpoint.apikey, options);
      } else if (endpointType === 'rkllama') {
        logger.info(`ü§ñ Criando cliente RKLLama para: ${endpoint.url}`);
        return new RKLlamaAPIClient(endpoint.url);
      } else {
        logger.info(`üß† Criando cliente Ollama para: ${endpoint.url}`);
        return new OllamaAPIClient(endpoint.url);
      }
    } catch (error) {
      logger.error(`‚ùå Erro ao criar cliente para ${endpoint.url}:`, error.message);
      throw error;
    }
  }

  async initialize() {
    logger.info('üîß Inicializando pool de APIs Ollama...');
    
    // Stop existing health check interval if running
    this.stopHealthCheckInterval();
    
    const effectiveConfig = await this.getEffectiveConfig();
    
    if (!effectiveConfig.ollamaApi.enabled || !effectiveConfig.ollamaApi.endpoints.length) {
      logger.warn('‚ö†Ô∏è OllamaAPI n√£o habilitado ou sem endpoints configurados');
      this.clients = [];
      return;
    }

    const clientPromises = effectiveConfig.ollamaApi.endpoints
      .filter(endpoint => endpoint.enabled)
      .map(async endpoint => {
        const client = await this.createClient(endpoint);
        client.endpoint = endpoint;
        client.retryCount = 0;
        
        const clientType = this.isRKLlamaEndpoint(endpoint) ? 'RKLLama' : 'Ollama';
        logger.info(`üì° Endpoint ${clientType} configurado: ${endpoint.url} (prioridade: ${endpoint.priority})`);
        return client;
      });

    this.clients = await Promise.all(clientPromises);

    this.clients.sort((a, b) => a.endpoint.priority - b.endpoint.priority);
    
    if (this.clients.length > 0) {
      logger.success(`‚úÖ Pool Ollama inicializado com ${this.clients.length} endpoints`);
      this.startHealthCheckInterval();
      
      // Load saved models for RKLLama endpoints
      await this.loadSavedModels();
    } else {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama habilitado encontrado');
    }
  }

  startHealthCheckInterval() {
    this.healthCheckInterval = setInterval(async () => {
      await this.performHealthChecks();
    }, CONFIG.ollamaApi.loadBalancing.healthCheckInterval);
  }

  stopHealthCheckInterval() {
    if (this.healthCheckInterval) {
      clearInterval(this.healthCheckInterval);
      this.healthCheckInterval = null;
      logger.debug('üõë Health check interval Ollama parado');
    }
  }

  async performHealthChecks() {
    // Check if the service is still enabled
    const isServiceEnabled = await this.isEnabled();
    if (!isServiceEnabled) {
      logger.debug('üõë OllamaAPI desabilitado, parando health checks');
      this.stopHealthCheckInterval();
      return;
    }

    if (this.clients.length === 0) {
      logger.debug('üõë Nenhum cliente Ollama dispon√≠vel para health check');
      return;
    }

    logger.debug(`üîç Executando health checks em ${this.clients.length} endpoints Ollama...`);
    
    const healthPromises = this.clients.map(async (client) => {
      try {
        logger.debug(`üîç Health check para ${client.baseURL}...`);
        await client.checkHealth();
        client.retryCount = 0;
        logger.debug(`‚úÖ Health check bem-sucedido para ${client.baseURL}`);
      } catch (error) {
        client.retryCount++;
        logger.warn(`‚ö†Ô∏è Health check falhou para ${client.baseURL} (tentativa ${client.retryCount}/${client.endpoint.maxRetries}): ${error.message}`);
        
        // Log detailed error for debugging
        if (error.message.includes('Invalid URL')) {
          logger.error(`‚ùå URL inv√°lida detectada para ${client.baseURL}:`, {
            originalURL: client.baseURL,
            endpointURL: client.endpoint?.url,
            error: error.message
          });
        }
      }
    });

    await Promise.allSettled(healthPromises);
    
    const healthyCount = this.getHealthyClients().length;
    logger.debug(`üìä Health check conclu√≠do: ${healthyCount}/${this.clients.length} endpoints saud√°veis`);
  }

  getHealthyClients() {
    return this.clients.filter(client => 
      client.isHealthy && client.retryCount < client.endpoint.maxRetries
    );
  }

  async selectClientByStrategy(healthyClients) {
    const effectiveConfig = await this.getEffectiveConfig();
    const strategy = effectiveConfig.ollamaApi.loadBalancing.strategy;
    
    logger.debug(`üéØ Aplicando estrat√©gia: ${strategy} para ${healthyClients.length} clientes`);
    
    switch (strategy) {
      case 'round_robin':
        return this.selectRoundRobin(healthyClients);
      
      case 'priority':
        return this.selectByPriority(healthyClients);
      
      case 'queue_length':
        return this.selectByLoad(healthyClients);
      
      case 'response_efficiency':
        return this.selectByResponseEfficiency(healthyClients);
      
      default:
        logger.warn(`‚ö†Ô∏è Estrat√©gia desconhecida: ${strategy}, usando priority`);
        return this.selectByPriority(healthyClients);
    }
  }

  selectRoundRobin(clients) {
    if (clients.length === 0) return null;
    
    const selectedIndex = this.currentIndex % clients.length;
    const client = clients[selectedIndex];
    this.currentIndex = (this.currentIndex + 1) % clients.length;
    
    logger.debug(`üîÑ Round-robin selecionou: ${client.baseURL} (√≠ndice: ${selectedIndex}/${clients.length})`);
    return client;
  }

  selectByPriority(clients) {
    if (clients.length === 0) return null;
    
    const client = clients[0];
    logger.debug(`‚≠ê Prioridade selecionou: ${client.baseURL} (prioridade: ${client.endpoint.priority})`);
    return client;
  }

  selectByLoad(clients) {
    if (clients.length === 0) return null;
    
    // Log informa√ß√µes de todos os clientes para debug
    const clientStats = clients.map(c => ({
      url: c.baseURL,
      type: c.constructor.name,
      activeRequests: c.activeRequests,
      totalRequests: c.totalRequests,
      runningModels: c.runningModels?.length || 0,
      loadScore: c.getLoadScore()
    }));
    logger.debug('üìä Estat√≠sticas de clientes para sele√ß√£o:', clientStats);
    
    const client = clients.reduce((best, current) => {
      const bestScore = best.getLoadScore();
      const currentScore = current.getLoadScore();
      return currentScore < bestScore ? current : best;
    });

    logger.debug(`üìä Load balancing selecionou: ${client.baseURL} (score: ${client.getLoadScore()}, ativo: ${client.activeRequests})`);
    return client;
  }

  selectByResponseEfficiency(clients) {
    if (clients.length === 0) return null;
    
    // Calcula efici√™ncia: Tempo estimado para completar pr√≥xima requisi√ß√£o
    const clientsWithEfficiency = clients.map(client => {
      const status = client.getProcessingStatus();
      const avgResponseTime = Math.max(status.averageResponseTime, 10); // M√≠nimo 10ms para evitar divis√£o por zero
      const queueSize = client.activeRequests;
      
      // Tempo estimado = (tempo m√©dio de resposta √ó (fila + 1))
      // +1 porque nossa requisi√ß√£o seria a pr√≥xima na fila
      const estimatedTime = avgResponseTime * (queueSize + 1);
      
      return {
        client,
        avgResponseTime,
        queueSize,
        estimatedTime,
        url: client.baseURL
      };
    });

    // Log para debug
    const debugInfo = clientsWithEfficiency.map(c => ({
      url: c.url,
      avgTime: c.avgResponseTime + 'ms',
      queue: c.queueSize,
      estimatedTime: c.estimatedTime + 'ms'
    }));
    logger.debug('üöÄ An√°lise de efici√™ncia de endpoints:', debugInfo);

    // Seleciona cliente com menor tempo estimado
    const bestClient = clientsWithEfficiency.reduce((best, current) => 
      current.estimatedTime < best.estimatedTime ? current : best
    );

    logger.debug(`üéØ Response efficiency selecionou: ${bestClient.url} (tempo estimado: ${bestClient.estimatedTime}ms, fila: ${bestClient.queueSize})`);
    return bestClient.client;
  }

  async selectBestClient() {
    const healthyClients = this.getHealthyClients();
    
    if (healthyClients.length === 0) {
      logger.error('‚ùå Nenhum endpoint Ollama saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    // Atualiza informa√ß√µes de carga para todos os clientes saud√°veis
    logger.debug('üìä Atualizando informa√ß√µes de carga para balanceamento...');
    await Promise.allSettled(
      healthyClients.map(async (client) => {
        try {
          await client.listRunningModels();
          const processingStatus = client.getProcessingStatus();
          logger.debug(`üìä ${client.baseURL}: ativo=${processingStatus.activeRequests}, total=${processingStatus.totalRequests}, score=${client.getLoadScore()}`);
        } catch (error) {
          logger.debug(`‚ö†Ô∏è Erro ao obter status de ${client.baseURL}: ${error.message}`);
        }
      })
    );

    const selectedClient = await this.selectClientByStrategy(healthyClients);
    const effectiveConfig = await this.getEffectiveConfig();
    logger.info(`üéØ Cliente selecionado: ${selectedClient.baseURL} (estrat√©gia: ${effectiveConfig.ollamaApi.loadBalancing.strategy})`);
    
    return selectedClient;
  }

  // ============ Generate Methods ============
  async generateWithFallback(options = {}) {
    const healthyClients = this.getHealthyClients();
    
    if (healthyClients.length === 0) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama API saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    let lastError;
    
    for (const client of healthyClients) {
      try {
        logger.info(`üéØ Tentando gera√ß√£o com ${client.baseURL}`);
        
        const result = await client.generate(options);
        
        logger.success(`‚úÖ Gera√ß√£o bem-sucedida via ${client.baseURL}`);
        return result;
        
      } catch (error) {
        lastError = error;
        client.retryCount++;
        
        logger.warn(`‚ö†Ô∏è Falha na gera√ß√£o via ${client.baseURL}: ${error.message}`);
        
        if (client.retryCount >= client.endpoint.maxRetries) {
          client.isHealthy = false;
          logger.error(`‚ùå Endpoint ${client.baseURL} marcado como n√£o saud√°vel ap√≥s ${client.retryCount} falhas`);
        }
        
        if (healthyClients.indexOf(client) < healthyClients.length - 1) {
          logger.info(`üîÑ Tentando pr√≥ximo endpoint em ${CONFIG.ollamaApi.retryDelay}ms...`);
          await new Promise(resolve => setTimeout(resolve, CONFIG.ollamaApi.retryDelay));
        }
      }
    }
    
    logger.error(`‚ùå Todos os endpoints Ollama API falharam. √öltimo erro: ${lastError?.message}`);
    throw new Error(`All Ollama API endpoints failed. Last error: ${lastError?.message}`);
  }

  async generate(options = {}) {
    logger.service('ü§ñ Iniciando gera√ß√£o via Ollama API...');
    
    // Use balanceamento adequado baseado na estrat√©gia configurada
    return await this.generateWithLoadBalancing(options);
  }

  async generateWithLoadBalancing(options = {}) {
    this.requestCount++;
    const effectiveConfig = await this.getEffectiveConfig();
    
    logger.info(`üéØ Requisi√ß√£o #${this.requestCount} - Iniciando balanceamento (estrat√©gia: ${effectiveConfig.ollamaApi.loadBalancing.strategy})`);
    
    const selectedClient = await this.selectBestClient();
    
    if (!selectedClient) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama API saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    // Override model if a specific model is configured for this endpoint
    const finalOptions = { ...options };
    if (selectedClient.endpoint.model) {
      const endpointType = this.getEndpointType(selectedClient.endpoint);
      
      // For ChatGPT endpoints, ensure we use OpenAI-compatible chat models
      if (endpointType === 'chatgpt') {
        const isValidChatModel = (selectedClient.endpoint.model.includes('gpt') || 
                                 selectedClient.endpoint.model.includes('o3') || 
                                 selectedClient.endpoint.model.includes('o4')) &&
                                !selectedClient.endpoint.model.includes('instruct'); // Instruct models are not chat models
        
        if (isValidChatModel) {
          finalOptions.model = selectedClient.endpoint.model;
          logger.debug(`üîß Usando modelo OpenAI espec√≠fico do endpoint: ${finalOptions.model}`);
        } else {
          // Use default GPT-4 for ChatGPT endpoints with invalid models
          finalOptions.model = 'gpt-4';
          logger.warn(`‚ö†Ô∏è Modelo ${selectedClient.endpoint.model} n√£o √© compat√≠vel com chat completions, usando gpt-4`);
        }
      } else {
        // For Ollama/RKLLama endpoints, use the configured model as-is
        finalOptions.model = selectedClient.endpoint.model;
        logger.debug(`üîß Usando modelo espec√≠fico do endpoint: ${finalOptions.model}`);
      }
    }

    const startTime = Date.now();
    try {
      const processingStatus = selectedClient.getProcessingStatus();
      logger.info(`üéØ Req #${this.requestCount}: Usando ${selectedClient.baseURL} (ativo: ${processingStatus.activeRequests}, score: ${selectedClient.getLoadScore()})`);
      
      // For ChatGPT endpoints, use different call format
      let result;
      const endpointType = this.getEndpointType(selectedClient.endpoint);
      if (endpointType === 'chatgpt') {
        // ChatGPT generate expects prompt as first parameter
        const prompt = finalOptions.prompt || '';
        const generateOptions = { ...finalOptions };
        delete generateOptions.prompt; // Remove prompt from options
        result = await selectedClient.generate(prompt, generateOptions);
      } else {
        // Ollama and RKLLama expect options object
        result = await selectedClient.generate(finalOptions);
      }
      
      const duration = Date.now() - startTime;
      logger.success(`‚úÖ Req #${this.requestCount}: Gera√ß√£o bem-sucedida via ${selectedClient.baseURL} em ${duration}ms`);
      return result;
      
    } catch (error) {
      selectedClient.retryCount++;
      const duration = Date.now() - startTime;
      logger.warn(`‚ö†Ô∏è Req #${this.requestCount}: Falha via ${selectedClient.baseURL} ap√≥s ${duration}ms: ${error.message}`);
      
      // Se falhar, marca como n√£o saud√°vel e tenta com fallback
      if (selectedClient.retryCount >= selectedClient.endpoint.maxRetries) {
        selectedClient.isHealthy = false;
        logger.error(`‚ùå Endpoint ${selectedClient.baseURL} marcado como n√£o saud√°vel ap√≥s ${selectedClient.retryCount} falhas`);
      }
      
      // Fallback para outros endpoints dispon√≠veis
      logger.info(`üîÑ Req #${this.requestCount}: Tentando fallback para outros endpoints...`);
      return await this.generateWithFallback(options);
    }
  }

  // ============ Chat Methods ============
  async chatWithFallback(options = {}) {
    const healthyClients = this.getHealthyClients();
    
    if (healthyClients.length === 0) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama API saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    let lastError;
    
    for (const client of healthyClients) {
      // Override model if a specific model is configured for this endpoint
      const finalOptions = { ...options };
      if (client.endpoint.model) {
        const endpointType = this.getEndpointType(client.endpoint);
        
        // For ChatGPT endpoints, ensure we use OpenAI-compatible chat models
        if (endpointType === 'chatgpt') {
          const isValidChatModel = (client.endpoint.model.includes('gpt') || 
                                   client.endpoint.model.includes('o3') || 
                                   client.endpoint.model.includes('o4')) &&
                                  !client.endpoint.model.includes('instruct'); // Instruct models are not chat models
          
          if (isValidChatModel) {
            finalOptions.model = client.endpoint.model;
            logger.debug(`üîß Usando modelo OpenAI espec√≠fico do endpoint para fallback: ${finalOptions.model}`);
          } else {
            // Use default GPT-4 for ChatGPT endpoints with invalid models
            finalOptions.model = 'gpt-4';
            logger.warn(`‚ö†Ô∏è Modelo ${client.endpoint.model} n√£o √© compat√≠vel com chat completions no fallback, usando gpt-4`);
          }
        } else {
          // For Ollama/RKLLama endpoints, use the configured model as-is
          finalOptions.model = client.endpoint.model;
          logger.debug(`üîß Usando modelo espec√≠fico do endpoint para fallback: ${finalOptions.model}`);
        }
      }

      try {
        logger.info(`üéØ Tentando chat com ${client.baseURL}`);
        
        // For ChatGPT endpoints, use different call format
        let result;
        const endpointType = this.getEndpointType(client.endpoint);
        if (endpointType === 'chatgpt') {
          // ChatGPT expects messages as first parameter
          const messages = finalOptions.messages || [];
          const chatOptions = { ...finalOptions };
          delete chatOptions.messages; // Remove messages from options
          result = await client.chat(messages, chatOptions);
        } else {
          // Ollama and RKLLama expect options object
          result = await client.chat(finalOptions);
        }
        
        logger.success(`‚úÖ Chat bem-sucedido via ${client.baseURL}`);
        return result;
        
      } catch (error) {
        lastError = error;
        client.retryCount++;
        
        logger.warn(`‚ö†Ô∏è Falha no chat via ${client.baseURL}: ${error.message}`);
        
        if (client.retryCount >= client.endpoint.maxRetries) {
          client.isHealthy = false;
          logger.error(`‚ùå Endpoint ${client.baseURL} marcado como n√£o saud√°vel ap√≥s ${client.retryCount} falhas`);
        }
        
        if (healthyClients.indexOf(client) < healthyClients.length - 1) {
          logger.info(`üîÑ Tentando pr√≥ximo endpoint em ${CONFIG.ollamaApi.retryDelay}ms...`);
          await new Promise(resolve => setTimeout(resolve, CONFIG.ollamaApi.retryDelay));
        }
      }
    }
    
    logger.error(`‚ùå Todos os endpoints Ollama API falharam. √öltimo erro: ${lastError?.message}`);
    throw new Error(`All Ollama API endpoints failed. Last error: ${lastError?.message}`);
  }

  async chat(options = {}) {
    logger.service('üí¨ Iniciando chat via Ollama API...');
    
    // Use balanceamento adequado baseado na estrat√©gia configurada
    return await this.chatWithLoadBalancing(options);
  }

  async chatWithLoadBalancing(options = {}) {
    this.requestCount++;
    const effectiveConfig = await this.getEffectiveConfig();
    
    logger.info(`üéØ Chat #${this.requestCount} - Iniciando balanceamento (estrat√©gia: ${effectiveConfig.ollamaApi.loadBalancing.strategy})`);
    
    const selectedClient = await this.selectBestClient();
    
    if (!selectedClient) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama API saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    // Override model if a specific model is configured for this endpoint
    const finalOptions = { ...options };
    if (selectedClient.endpoint.model) {
      const endpointType = this.getEndpointType(selectedClient.endpoint);
      
      // For ChatGPT endpoints, ensure we use OpenAI-compatible chat models
      if (endpointType === 'chatgpt') {
        const isValidChatModel = (selectedClient.endpoint.model.includes('gpt') || 
                                 selectedClient.endpoint.model.includes('o3') || 
                                 selectedClient.endpoint.model.includes('o4')) &&
                                !selectedClient.endpoint.model.includes('instruct'); // Instruct models are not chat models
        
        if (isValidChatModel) {
          finalOptions.model = selectedClient.endpoint.model;
          logger.debug(`üîß Usando modelo OpenAI espec√≠fico do endpoint: ${finalOptions.model}`);
        } else {
          // Use default GPT-4 for ChatGPT endpoints with invalid models
          finalOptions.model = 'gpt-4';
          logger.warn(`‚ö†Ô∏è Modelo ${selectedClient.endpoint.model} n√£o √© compat√≠vel com chat completions, usando gpt-4`);
        }
      } else {
        // For Ollama/RKLLama endpoints, use the configured model as-is
        finalOptions.model = selectedClient.endpoint.model;
        logger.debug(`üîß Usando modelo espec√≠fico do endpoint: ${finalOptions.model}`);
      }
    }

    const startTime = Date.now();
    try {
      const processingStatus = selectedClient.getProcessingStatus();
      logger.info(`üéØ Chat #${this.requestCount}: Usando ${selectedClient.baseURL} (ativo: ${processingStatus.activeRequests}, score: ${selectedClient.getLoadScore()})`);
      
      // For ChatGPT endpoints, use different call format
      let result;
      const endpointType = this.getEndpointType(selectedClient.endpoint);
      if (endpointType === 'chatgpt') {
        // ChatGPT expects messages as first parameter
        const messages = finalOptions.messages || [];
        const chatOptions = { ...finalOptions };
        delete chatOptions.messages; // Remove messages from options
        result = await selectedClient.chat(messages, chatOptions);
      } else {
        // Ollama and RKLLama expect options object
        result = await selectedClient.chat(finalOptions);
      }
      
      const duration = Date.now() - startTime;
      logger.success(`‚úÖ Chat #${this.requestCount}: Chat bem-sucedido via ${selectedClient.baseURL} em ${duration}ms`);
      return {
        result,
        endpoint: selectedClient.baseURL,
        client: selectedClient,
        duration
      };
      
    } catch (error) {
      selectedClient.retryCount++;
      const duration = Date.now() - startTime;
      logger.warn(`‚ö†Ô∏è Chat #${this.requestCount}: Falha via ${selectedClient.baseURL} ap√≥s ${duration}ms: ${error.message}`);
      
      // Se falhar, marca como n√£o saud√°vel e tenta com fallback
      if (selectedClient.retryCount >= selectedClient.endpoint.maxRetries) {
        selectedClient.isHealthy = false;
        logger.error(`‚ùå Endpoint ${selectedClient.baseURL} marcado como n√£o saud√°vel ap√≥s ${selectedClient.retryCount} falhas`);
      }
      
      // Fallback para outros endpoints dispon√≠veis
      logger.info(`üîÑ Chat #${this.requestCount}: Tentando fallback para outros endpoints...`);
      return await this.chatWithFallback(options);
    }
  }

  async chatWithSpecificEndpoint(endpointUrl, options = {}) {
    // Encontra o cliente espec√≠fico pelo URL
    const client = this.clients.find(c => c.baseURL === endpointUrl);
    
    if (!client) {
      throw new Error(`Endpoint ${endpointUrl} n√£o encontrado no pool`);
    }
    
    // Verifica se o cliente est√° saud√°vel (isHealthy √© uma propriedade, n√£o m√©todo)
    if (!client.isHealthy || client.retryCount >= client.endpoint.maxRetries) {
      throw new Error(`Endpoint ${endpointUrl} n√£o est√° saud√°vel (health: ${client.isHealthy}, retries: ${client.retryCount}/${client.endpoint.maxRetries})`);
    }
    
    const startTime = Date.now();
    try {
      logger.info(`üéØ Chat direto: Usando endpoint espec√≠fico ${endpointUrl}`);
      const result = await client.chat(options);
      
      const duration = Date.now() - startTime;
      logger.success(`‚úÖ Chat direto: Sucesso via ${endpointUrl} em ${duration}ms`);
      return result;
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error(`‚ùå Chat direto: Falha via ${endpointUrl} ap√≥s ${duration}ms:`, error);
      throw error;
    }
  }

  async chatWithSpecificEndpointAndModel(endpointUrl, model, options = {}) {
    // Encontra o cliente espec√≠fico pelo URL
    const client = this.clients.find(c => c.baseURL === endpointUrl);
    
    if (!client) {
      throw new Error(`Endpoint ${endpointUrl} n√£o encontrado no pool`);
    }
    
    // Verifica se o cliente est√° saud√°vel
    if (!client.isHealthy || client.retryCount >= client.endpoint.maxRetries) {
      throw new Error(`Endpoint ${endpointUrl} n√£o est√° saud√°vel (health: ${client.isHealthy}, retries: ${client.retryCount}/${client.endpoint.maxRetries})`);
    }
    
    const startTime = Date.now();
    try {
      logger.info(`üéØ Chat direto: Usando endpoint espec√≠fico ${endpointUrl} com modelo ${model}`);
      
      // Define o modelo espec√≠fico no options
      const chatOptions = {
        model: model,
        stream: false,
        ...options
      };
      
      // Use m√©todo espec√≠fico para ChatGPT se dispon√≠vel
      let result;
      if (client.chatWithSpecificEndpointAndModel && this.isChatGPTEndpoint(client.endpoint)) {
        result = await client.chatWithSpecificEndpointAndModel(endpointUrl, model, chatOptions);
      } else {
        result = await client.chat(chatOptions);
      }
      
      const duration = Date.now() - startTime;
      logger.success(`‚úÖ Chat direto: Sucesso via ${endpointUrl} com modelo ${model} em ${duration}ms`);
      return result;
    } catch (error) {
      const duration = Date.now() - startTime;
      logger.error(`‚ùå Chat direto: Falha via ${endpointUrl} com modelo ${model} ap√≥s ${duration}ms:`, error);
      throw error;
    }
  }

  // ============ Model Management Methods ============
  async listModels() {
    try {
      const client = await this.selectBestClient();
      return await client.listModels();
    } catch (error) {
      return await this.executeWithFallback('listModels');
    }
  }

  async listModelsFromAllEndpoints() {
    logger.info('üîç Listando modelos de todos os endpoints Ollama...');
    
    if (this.clients.length === 0) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint configurado');
      return { endpoints: [], totalModels: 0, uniqueModels: [] };
    }

    const results = [];
    const uniqueModels = new Set();
    
    for (const client of this.clients) {
      const endpointResult = {
        url: client.endpoint.url,
        type: this.isRKLlamaEndpoint(client.endpoint) ? 'RKLLama' : 'Ollama',
        priority: client.endpoint.priority,
        healthy: false,
        models: [],
        error: null
      };

      try {
        logger.debug(`üì° Consultando modelos do endpoint: ${client.endpoint.url}`);
        const response = await client.listModels();
        
        if (response && response.models) {
          endpointResult.healthy = true;
          endpointResult.models = response.models;
          
          // Adicionar modelos √∫nicos ao conjunto
          response.models.forEach(model => {
            uniqueModels.add(model.name);
          });
          
          logger.success(`‚úÖ ${response.models.length} modelos encontrados em ${client.endpoint.url}`);
        } else {
          endpointResult.error = 'Resposta inv√°lida do endpoint';
          logger.warn(`‚ö†Ô∏è Resposta inv√°lida do endpoint ${client.endpoint.url}`);
        }
      } catch (error) {
        endpointResult.error = error.message;
        logger.error(`‚ùå Erro ao listar modelos do endpoint ${client.endpoint.url}:`, error.message);
      }

      results.push(endpointResult);
    }

    const totalModels = results.reduce((sum, result) => sum + result.models.length, 0);
    
    logger.info(`üìä Resumo: ${totalModels} modelos totais, ${uniqueModels.size} √∫nicos em ${results.length} endpoints`);
    
    return {
      endpoints: results,
      totalModels,
      uniqueModels: Array.from(uniqueModels).sort(),
      timestamp: new Date().toISOString()
    };
  }

  async showModel(model, verbose = false) {
    try {
      const client = await this.selectBestClient();
      return await client.showModel(model, verbose);
    } catch (error) {
      return await this.executeWithFallback('showModel', model, verbose);
    }
  }

  async pullModel(model, stream = false) {
    try {
      const client = await this.selectBestClient();
      return await client.pullModel(model, stream);
    } catch (error) {
      return await this.executeWithFallback('pullModel', model, stream);
    }
  }

  async createModel(options = {}) {
    try {
      const client = await this.selectBestClient();
      return await client.createModel(options);
    } catch (error) {
      return await this.executeWithFallback('createModel', options);
    }
  }

  async deleteModel(model) {
    try {
      const client = await this.selectBestClient();
      return await client.deleteModel(model);
    } catch (error) {
      return await this.executeWithFallback('deleteModel', model);
    }
  }

  async copyModel(source, destination) {
    try {
      const client = await this.selectBestClient();
      return await client.copyModel(source, destination);
    } catch (error) {
      return await this.executeWithFallback('copyModel', source, destination);
    }
  }

  // ============ Other Methods ============
  async generateEmbeddings(model, input, options = {}) {
    try {
      const client = await this.selectBestClient();
      return await client.generateEmbeddings(model, input, options);
    } catch (error) {
      return await this.executeWithFallback('generateEmbeddings', model, input, options);
    }
  }

  async listRunningModels() {
    try {
      const client = await this.selectBestClient();
      return await client.listRunningModels();
    } catch (error) {
      return await this.executeWithFallback('listRunningModels');
    }
  }

  // ============ Generic Fallback Method ============
  async executeWithFallback(method, ...args) {
    const healthyClients = this.getHealthyClients();
    
    if (healthyClients.length === 0) {
      logger.warn('‚ö†Ô∏è Nenhum endpoint Ollama API saud√°vel dispon√≠vel');
      throw new Error('No healthy Ollama API endpoints available');
    }

    let lastError;
    
    for (const client of healthyClients) {
      try {
        logger.debug(`üéØ Tentando ${method} com ${client.baseURL}`);
        const result = await client[method](...args);
        logger.success(`‚úÖ ${method} bem-sucedido via ${client.baseURL}`);
        return result;
      } catch (error) {
        lastError = error;
        client.retryCount++;
        
        logger.warn(`‚ö†Ô∏è Falha em ${method} via ${client.baseURL}: ${error.message}`);
        
        if (client.retryCount >= client.endpoint.maxRetries) {
          client.isHealthy = false;
          logger.error(`‚ùå Endpoint ${client.baseURL} marcado como n√£o saud√°vel ap√≥s ${client.retryCount} falhas`);
        }
        
        if (healthyClients.indexOf(client) < healthyClients.length - 1) {
          logger.debug(`üîÑ Tentando pr√≥ximo endpoint para ${method}...`);
          await new Promise(resolve => setTimeout(resolve, CONFIG.ollamaApi.retryDelay));
        }
      }
    }
    
    logger.error(`‚ùå Todos os endpoints falharam para ${method}. √öltimo erro: ${lastError?.message}`);
    throw new Error(`All Ollama API endpoints failed. Last error: ${lastError?.message}`);
  }

  // ============ Pool Status ============
  async getPoolStatus() {
    const effectiveConfig = await this.getEffectiveConfig();
    const status = {
      enabled: effectiveConfig.ollamaApi.enabled,
      mode: effectiveConfig.ollamaApi.mode || 'local',
      totalEndpoints: this.clients.length,
      healthyEndpoints: 0, // Will be calculated after health checks
      strategy: CONFIG.ollamaApi.loadBalancing.strategy,
      endpoints: []
    };

    for (const client of this.clients) {
      try {
        const health = await client.getHealth();
        const runningModels = await client.listRunningModels();
        const clientType = this.getEndpointType(client.endpoint);
        
        const processingStatus = client.getProcessingStatus ? client.getProcessingStatus() : {
          activeRequests: 0,
          totalRequests: 0,
          recentRequests: 0,
          averageResponseTime: 0,
          requestHistory: []
        };

        status.endpoints.push({
          url: client.baseURL,
          type: clientType,
          healthy: client.isHealthy,
          priority: client.endpoint.priority,
          runningModels: runningModels.models?.length || 0,
          loadScore: client.getLoadScore(),
          retryCount: client.retryCount,
          lastHealthCheck: client.lastHealthCheck ? new Date(client.lastHealthCheck).toISOString() : 'never',
          version: health.version,
          currentModel: health.currentModel || null,
          processing: processingStatus
        });
      } catch (error) {
        const clientType = this.getEndpointType(client.endpoint);
        const processingStatus = client.getProcessingStatus ? client.getProcessingStatus() : {
          activeRequests: 0,
          totalRequests: 0,
          recentRequests: 0,
          averageResponseTime: 0,
          requestHistory: []
        };

        status.endpoints.push({
          url: client.baseURL,
          type: clientType,
          healthy: false,
          priority: client.endpoint.priority,
          error: error.message,
          retryCount: client.retryCount,
          lastHealthCheck: client.lastHealthCheck ? new Date(client.lastHealthCheck).toISOString() : 'never',
          processing: processingStatus
        });
      }
    }

    // Calculate healthy endpoints after all health checks are done
    status.healthyEndpoints = this.getHealthyClients().length;

    return status;
  }

  async isEnabled() {
    const effectiveConfig = await this.getEffectiveConfig();
    return effectiveConfig.ollamaApi.enabled && this.clients.length > 0;
  }

  hasHealthyEndpoints() {
    return this.getHealthyClients().length > 0;
  }

  getMode() {
    return CONFIG.ollamaApi.mode;
  }

  async reinitialize() {
    logger.info('üîÑ Reinicializando pool de APIs Ollama...');
    await this.initialize();
  }

  async loadSavedModels() {
    if (!this.configService) {
      logger.debug('üîß ConfigService n√£o dispon√≠vel, pulando carregamento de modelos salvos');
      return;
    }

    try {
      const config = await this.configService.getConfig();
      const endpoints = config?.ollamaApi?.endpoints || [];
      
      logger.info('üîÑ Carregando modelos salvos para todos os endpoints...');
      
      for (const client of this.clients) {
        const endpoint = client.endpoint; // Get the endpoint config directly from the client

        // Load models for both Ollama and RKLLama endpoints
        if (endpoint.model && endpoint.enabled) {
          try {
            const endpointType = this.getEndpointType(endpoint);
            
            // ChatGPT endpoints don't need model loading/pulling
            if (endpointType === 'chatgpt') {
              logger.info(`üöÄ Endpoint ChatGPT ${endpoint.url} configurado com modelo ${endpoint.model} (sem pr√©-carregamento necess√°rio)`);
              continue;
            }
            
            const endpointTypeName = endpointType === 'rkllama' ? 'RKLLama' : 'Ollama';
            logger.info(`üîÑ Carregando modelo salvo ${endpoint.model} para ${endpointTypeName} ${endpoint.url}`);

            // Check if endpoint is healthy first
            await client.checkHealth();

            if (client.isHealthy) {
              if (endpointType === 'rkllama') {
                // For RKLLama, use loadModel method
                await client.loadModel(endpoint.model);
                logger.success(`‚úÖ Modelo ${endpoint.model} carregado automaticamente em RKLLama ${endpoint.url}`);
              } else {
                // For Ollama, use pullModel method
                await client.pullModel(endpoint.model);
                logger.success(`‚úÖ Modelo ${endpoint.model} pr√©-carregado automaticamente em Ollama ${endpoint.url}`);
              }
            } else {
              logger.warn(`‚ö†Ô∏è Endpoint ${endpoint.url} n√£o est√° saud√°vel, pulando carregamento do modelo ${endpoint.model}`);
            }
          } catch (error) {
            const endpointType = this.getEndpointType(endpoint);
            const endpointTypeName = endpointType === 'rkllama' ? 'RKLLama' : endpointType === 'chatgpt' ? 'ChatGPT' : 'Ollama';
            logger.warn(`‚ö†Ô∏è Falha ao carregar modelo ${endpoint.model} para ${endpointTypeName} ${endpoint.url}: ${error.message}`);
          }
        }
      }
      
      logger.info('‚úÖ Carregamento de modelos salvos conclu√≠do');
    } catch (error) {
      logger.error('‚ùå Erro ao carregar modelos salvos:', error);
    }
  }

  destroy() {
    logger.info('üóëÔ∏è Destruindo pool de APIs Ollama...');
    this.stopHealthCheckInterval();
    this.clients = [];
  }
}

export default OllamaAPIPool;