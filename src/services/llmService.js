import { Ollama } from 'ollama';
import Utils from '../utils/index.js'; // Ajustar caminho se necess√°rio
import { CONFIG, CHAT_MODES, PROMPTS, getDynamicConfig } from '../config/index.js'; // Ajustar caminho se necess√°rio
import { fetchProfileStructured } from './linkedinScraper.js';
import JobQueue from './jobQueue.js';
import OllamaAPIPool from './ollamaApiPool.js';
import logger from '../utils/logger.js';

// ============ Servi√ßo LLM ============
class LLMService {
  constructor(configService = null, sessionService = null) {
    this.configService = configService;
    this.sessionService = sessionService;
    this.contexts = new Map(); // Cache local para performance
    this.ollamaApiPool = new OllamaAPIPool(configService);
    this.queue = new JobQueue(
      CONFIG.queues.llmConcurrency,
      CONFIG.queues.memoryThresholdGB
    );
    this.ollama = null; // Initialize after config is loaded
    this.loadLastUsedModel();
    this.initializeOllama();

    // Configura√ß√µes de timeout progressivo
    this.timeoutLevels = [
      6000000,   // 1 minuto
      180000000, // 30 minutos
      360000000  // 1 hora (limite m√°ximo)
    ];
  }

  initializeOllama() {
    this.ollama = new Ollama({ host: CONFIG.llm.host });
    logger.info(`Ollama client initialized with host: ${CONFIG.llm.host}`);
  }

  async loadLastUsedModel() {
    if (this.configService) {
      try {
        const config = await this.configService.getConfig();
        if (config) {
          // Priorizar configura√ß√£o local do OllamaAPI se estiver em modo local
          let modelToUse = null;
          let hostToUse = null;
          
          if (config.ollamaApi?.mode === 'local') {
            if (config.ollamaApi?.localModel) {
              modelToUse = config.ollamaApi.localModel;
              logger.info(`üè† Modo local detectado - usando modelo local: ${modelToUse}`);
            }
            
            if (config.ollamaApi?.localPort) {
              hostToUse = `http://localhost:${config.ollamaApi.localPort}`;
              logger.info(`üîå Usando porta local configurada: ${config.ollamaApi.localPort}`);
            }
            
            if (config.ollamaApi?.localProtocol) {
              logger.info(`ü§ñ Protocolo local: ${config.ollamaApi.localProtocol}`);
            }
          } else if (config.llm?.model) {
            modelToUse = config.llm.model;
            logger.info(`üåê Usando modelo padr√£o: ${modelToUse}`);
          }
          
          if (modelToUse) {
            CONFIG.llm.model = modelToUse; // Update global CONFIG
            logger.info(`‚úÖ Modelo LLM carregado do MongoDB: ${CONFIG.llm.model}`);
          }
          
          if (hostToUse) {
            CONFIG.llm.host = hostToUse; // Update global CONFIG
            this.initializeOllama(); // Reinitialize with new host
            logger.info(`‚úÖ Host LLM atualizado: ${CONFIG.llm.host}`);
          }
        }
      } catch (error) {
        logger.warn('‚ö†Ô∏è Erro ao carregar configura√ß√£o LLM do MongoDB, usando configura√ß√£o padr√£o:', error.message);
      }
    }
  }

  async getEffectiveConfig() {
    let mongoConfig = null;
    if (this.configService) {
      try {
        mongoConfig = await this.configService.getConfig();
      } catch (error) {
        logger.warn('‚ö†Ô∏è Erro ao obter configura√ß√£o do MongoDB para LLM, usando configura√ß√£o padr√£o:', error.message);
      }
    }
    return getDynamicConfig(mongoConfig);
  }

  async shouldUseApiPool() {
    const config = await this.getEffectiveConfig();
    return config.ollamaApi.enabled && 
           config.ollamaApi.mode === 'api' && 
           await this.ollamaApiPool.isEnabled() &&
           this.ollamaApiPool.hasHealthyEndpoints();
  }

  async getContext(contactId, type) {
    const key = `${contactId}_${type}`;
    
    // Verifica cache local primeiro
    if (this.contexts.has(key)) {
      return this.contexts.get(key);
    }
    
    // Carrega da sess√£o persistida se dispon√≠vel
    if (this.sessionService) {
      try {
        const session = await this.sessionService.getSession(contactId);
        if (session && session.llmContext && session.llmContext[type]) {
          const persistedContext = session.llmContext[type];
          this.contexts.set(key, persistedContext);
          logger.debug(`üíº Contexto LLM carregado da sess√£o para ${contactId} modo ${type} (${persistedContext.length} mensagens)`);
          return persistedContext;
        }
      } catch (error) {
        logger.error(`‚ùå Erro ao carregar contexto da sess√£o para ${contactId}:`, error);
      }
    }
    
    // Cria contexto vazio se n√£o existe
    const newContext = [];
    this.contexts.set(key, newContext);
    return newContext;
  }

  async chat(contactId, text, type, systemPrompt, maxRetries = this.timeoutLevels.length) {
    const context = await this.getContext(contactId, type);
    context.push({ role: 'user', content: text });
    
    // Usa o m√©todo est√°tico de Utils para limitar o contexto
    const limitedContext = Utils.limitContext([...context]); 
    const messages = [{ role: 'system', content: systemPrompt }, ...limitedContext];
    
    for (let attempt = 0; attempt < maxRetries; attempt++) {
      const timeoutMs = this.timeoutLevels[attempt] || this.timeoutLevels[this.timeoutLevels.length - 1];
      const timeoutLabel = this.formatTimeout(timeoutMs);
      
      try {
        logger.service(`üîÑ LLM Tentativa ${attempt + 1}/${maxRetries} (timeout: ${timeoutLabel}) para ${contactId}`);
        
        const response = await this.queue.add(() => 
          this.chatWithTimeout({
            model: CONFIG.llm.model,
            messages
          }, timeoutMs)
        );
        
        // Usa o m√©todo est√°tico de Utils para extrair JSON
        const content = type === CHAT_MODES.AGENDABOT 
          ? Utils.extractJSON(response.message.content)
          : response.message.content;
        
        context.push({ role: 'assistant', content });
        
        // Persiste o contexto atualizado na sess√£o
        if (this.sessionService) {
          try {
            await this.sessionService.saveLLMContext(contactId, type, context);
          } catch (error) {
            logger.error(`‚ùå Erro ao persistir contexto LLM para ${contactId}:`, error);
          }
        }
        
        logger.success(`‚úÖ LLM resposta obtida em tentativa ${attempt + 1} para ${contactId}`);
        return content;
      } catch (err) {
        const isTimeout = err.code === 'UND_ERR_HEADERS_TIMEOUT' || err.name === 'TimeoutError' || err.message?.includes('timeout');
        
        logger.error(`‚ùå LLM (${type}) - Tentativa ${attempt + 1}/${maxRetries} [${timeoutLabel}]:`, {
          error: err.message,
          code: err.code,
          isTimeout,
          contactId
        });
        
        if (attempt === maxRetries - 1) {
          // Remove the failed user message from context on final failure
          context.pop();
          logger.error(`üö´ LLM falhou definitivamente ap√≥s ${maxRetries} tentativas para ${contactId}`);
          throw new Error(`LLM falhou ap√≥s ${maxRetries} tentativas. √öltimo erro: ${err.message}`);
        }
        
        // Delay progressivo entre tentativas
        const delayMs = Math.min(2000 * Math.pow(1.5, attempt), 30000);
        logger.info(`‚è≥ Aguardando ${this.formatTimeout(delayMs)} antes da pr√≥xima tentativa...`);
        await new Promise(resolve => setTimeout(resolve, delayMs));
      }
    }
  }
  
  async chatWithTimeout(requestParams, timeoutMs) {
    return new Promise(async (resolve, reject) => {
      const timeout = setTimeout(() => {
        reject(new Error(`LLM timeout ap√≥s ${this.formatTimeout(timeoutMs)}`));
      }, timeoutMs);
      
      try {
        const useApiPool = await this.shouldUseApiPool();
        let response;
        
        if (useApiPool) {
          logger.debug('üîÑ Usando Ollama API Pool para chat');
          response = await this.ollamaApiPool.chat({
            ...requestParams,
            stream: false
          });
        } else {
          logger.debug('üîÑ Usando Ollama local para chat');
          response = await this.ollama.chat(requestParams);
        }
        
        clearTimeout(timeout);
        resolve(response);
      } catch (error) {
        clearTimeout(timeout);
        
        // Se falhou com API Pool, sempre tenta fallback para local
        const usedApiPool = await this.shouldUseApiPool();
        if (usedApiPool) {
          try {
            logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback...');
            const response = await this.ollama.chat(requestParams);
            logger.success('‚úÖ Fallback para Ollama local bem-sucedido');
            resolve(response);
            return;
          } catch (fallbackError) {
            logger.error('‚ùå Fallback para Ollama local tamb√©m falhou:', fallbackError.message);
            // Use the more informative error message
            const combinedError = new Error(`API pool failed: ${error.message}. Local fallback failed: ${fallbackError.message}`);
            reject(combinedError);
            return;
          }
        }
        
        reject(error);
      }
    });
  }
  
  formatTimeout(ms) {
    if (ms < 60000) {
      return `${Math.round(ms / 1000)}s`;
    } else if (ms < 3600000) {
      return `${Math.round(ms / 60000)}min`;
    } else {
      return `${Math.round(ms / 3600000)}h`;
    }
  }

  async getChatGPTResponse(contactId, text) {
    // Usa o m√©todo est√°tico de Utils para obter a data
    const date = Utils.getCurrentDateInGMTMinus3().toISOString();
    return this.chat(contactId, text, CHAT_MODES.AGENDABOT, PROMPTS.agenda(date));
  }

  async getAssistantResponse(contactId, text) {
    // Usa o m√©todo est√°tico de Utils para obter a data
    const date = Utils.getCurrentDateInGMTMinus3().toISOString();
    return this.chat(contactId, text, CHAT_MODES.ASSISTANT, PROMPTS.assistant(date));
  }

  async chatWithSpecificEndpoint(contactId, text, endpointUrl) {
    // Usar endpoint espec√≠fico para resposta do assistente
    const date = Utils.getCurrentDateInGMTMinus3().toISOString();
    const context = await this.getContext(contactId, CHAT_MODES.ASSISTANT);
    context.push({ role: 'user', content: text });
    
    const limitedContext = Utils.limitContext([...context]); 
    const messages = [{ role: 'system', content: PROMPTS.assistant(date) }, ...limitedContext];
    
    try {
      // Usar o pool de APIs para fazer uma requisi√ß√£o espec√≠fica ao endpoint
      const response = await this.ollamaApiPool.chatWithSpecificEndpoint(endpointUrl, {
        messages: messages
      });
      
      const content = response.message.content;
      context.push({ role: 'assistant', content });
      
      // Persiste o contexto atualizado na sess√£o
      if (this.sessionService) {
        try {
          await this.sessionService.saveLLMContext(contactId, CHAT_MODES.ASSISTANT, context);
        } catch (error) {
          logger.error(`‚ùå Erro ao persistir contexto LLM para ${contactId}:`, error);
        }
      }
      
      logger.success(`‚úÖ LLM resposta obtida do endpoint espec√≠fico ${endpointUrl} para ${contactId}`);
      return content;
    } catch (error) {
      // Remove mensagem do usu√°rio em caso de falha
      context.pop();
      logger.error(`‚ùå Erro no endpoint espec√≠fico ${endpointUrl} para ${contactId}:`, error);
      throw error;
    }
  }

  async chatWithSpecificEndpointAndModel(contactId, text, endpointUrl, model) {
    // Usar endpoint espec√≠fico com modelo espec√≠fico para resposta do assistente
    const date = Utils.getCurrentDateInGMTMinus3().toISOString();
    const context = await this.getContext(contactId, CHAT_MODES.ASSISTANT);
    context.push({ role: 'user', content: text });
    
    const limitedContext = Utils.limitContext([...context]); 
    const messages = [{ role: 'system', content: PROMPTS.assistant(date) }, ...limitedContext];
    
    try {
      // Usar o pool de APIs para fazer uma requisi√ß√£o espec√≠fica ao endpoint com modelo espec√≠fico
      const response = await this.ollamaApiPool.chatWithSpecificEndpointAndModel(endpointUrl, model, {
        messages: messages
      });
      
      const content = response.message.content;
      context.push({ role: 'assistant', content });
      
      // Persiste o contexto atualizado na sess√£o
      if (this.sessionService) {
        try {
          await this.sessionService.saveLLMContext(contactId, CHAT_MODES.ASSISTANT, context);
        } catch (error) {
          logger.error(`‚ùå Erro ao persistir contexto LLM para ${contactId}:`, error);
        }
      }
      
      logger.success(`‚úÖ LLM resposta obtida do endpoint espec√≠fico ${endpointUrl} com modelo ${model} para ${contactId}`);
      return content;
    } catch (error) {
      // Remove mensagem do usu√°rio em caso de falha
      context.pop();
      logger.error(`‚ùå Erro no endpoint espec√≠fico ${endpointUrl} com modelo ${model} para ${contactId}:`, error);
      throw error;
    }
  }

  async getAssistantResponseLinkedin(contactId, url, liAt) {
    try {
      const data = await fetchProfileStructured(url, {
        liAt,
        timeoutMs: CONFIG.linkedin.timeoutMs,
        retries: 3
      });
      
      if (!data.success) {
        return `‚ùå Erro ao analisar perfil: ${data.error}`;
      }

      const profileData = data.data;
      const quality = data.dataQuality;

      const structuredText = this.formatProfileForLLM(profileData, quality);
      
      const enhancedPrompt = `${PROMPTS.linkedin}

QUALIDADE DOS DADOS: ${quality.quality} (${quality.percentage}%)
TENTATIVAS: ${data.attempt || 1}

Analise o perfil abaixo e forne√ßa um resumo profissional detalhado e insights relevantes:

${structuredText}`;

      return await this.chat(contactId, enhancedPrompt, CHAT_MODES.LINKEDIN, PROMPTS.linkedin);
    } catch (err) {
      logger.error('Erro ao raspar LinkedIn:', err);
      return '‚ùå Erro interno ao processar perfil LinkedIn.';
    }
  }

  formatProfileForLLM(profileData, quality) {
    let formatted = '';
    
    if (profileData.name) {
      formatted += `**NOME:** ${profileData.name}\n`;
    }
    
    if (profileData.headline) {
      formatted += `**T√çTULO:** ${profileData.headline}\n`;
    }
    
    if (profileData.location) {
      formatted += `**LOCALIZA√á√ÉO:** ${profileData.location}\n`;
    }
    
    if (profileData.connections) {
      formatted += `**CONEX√ïES:** ${profileData.connections}\n`;
    }
    
    if (profileData.about && profileData.about.length > 0) {
      formatted += `\n**SOBRE:**\n${profileData.about}\n`;
    }
    
    if (profileData.experience && profileData.experience.length > 0) {
      formatted += `\n**EXPERI√äNCIA PROFISSIONAL:**\n`;
      profileData.experience.forEach((exp, index) => {
        formatted += `${index + 1}. ${exp.title || 'Cargo n√£o especificado'}\n`;
        formatted += `   Empresa: ${exp.company || 'Empresa n√£o especificada'}\n`;
        formatted += `   Per√≠odo: ${exp.duration || 'Per√≠odo n√£o especificado'}\n\n`;
      });
    }
    
    if (profileData.education && profileData.education.length > 0) {
      formatted += `**EDUCA√á√ÉO:**\n`;
      profileData.education.forEach((edu, index) => {
        formatted += `${index + 1}. ${edu.degree || 'Curso n√£o especificado'}\n`;
        formatted += `   Institui√ß√£o: ${edu.school || 'Institui√ß√£o n√£o especificada'}\n`;
        formatted += `   Per√≠odo: ${edu.years || 'Per√≠odo n√£o especificado'}\n\n`;
      });
    }
    
    if (profileData.skills && profileData.skills.length > 0) {
      formatted += `**PRINCIPAIS HABILIDADES:**\n`;
      formatted += profileData.skills.slice(0, 15).join(' ‚Ä¢ ') + '\n';
    }
    
    formatted += `\n**QUALIDADE DOS DADOS EXTRA√çDOS:** ${quality.percentage}% (${quality.score}/${quality.maxScore} campos preenchidos)`;
    
    return formatted;
  }

  async clearContext(contactId, type) {
    const key = `${contactId}_${type}`;
    this.contexts.delete(key);
    
    // Limpa tamb√©m da sess√£o persistida
    if (this.sessionService) {
      try {
        await this.sessionService.clearLLMContext(contactId, type);
        logger.debug(`üßπ Contexto LLM limpo da sess√£o para ${contactId} modo ${type}`);
      } catch (error) {
        logger.error(`‚ùå Erro ao limpar contexto da sess√£o para ${contactId}:`, error);
      }
    }
  }

  // M√©todo para gerar resposta gen√©rica (usado pelo Telegram)
  async generateResponse(prompt, options = {}) {
    const contactId = 'telegram_' + Date.now();
    const maxTokens = options.maxTokens || 2000;
    const temperature = options.temperature || 0.7;
    
    try {
      const useApiPool = await this.shouldUseApiPool();
      let response;
      
      const requestParams = {
        model: CONFIG.llm.model,
        messages: [{ role: 'user', content: prompt }],
        options: {
          temperature: temperature
        },
        stream: false
      };
      
      if (useApiPool) {
        logger.debug('üîÑ Usando Ollama API Pool para gera√ß√£o de resposta');
        response = await this.ollamaApiPool.chat(requestParams);
      } else {
        logger.debug('üîÑ Usando Ollama local para gera√ß√£o de resposta');
        response = await this.ollama.chat(requestParams);
      }
      
      return response.message.content;
    } catch (error) {
      // Fallback para local se a API falhar
      const usedApiPool = await this.shouldUseApiPool();
      if (usedApiPool) {
        try {
          logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback na gera√ß√£o de resposta...');
          const response = await this.ollama.chat({
            model: CONFIG.llm.model,
            messages: [{ role: 'user', content: prompt }],
            options: {
              temperature: temperature
            }
          });
          logger.success('‚úÖ Fallback para Ollama local bem-sucedido na gera√ß√£o de resposta');
          return response.message.content;
        } catch (fallbackError) {
          logger.error('‚ùå Fallback tamb√©m falhou na gera√ß√£o de resposta:', fallbackError.message);
          throw new Error(`API pool failed: ${error.message}. Local fallback failed: ${fallbackError.message}`);
        }
      }
      
      logger.error('Erro ao gerar resposta LLM:', error);
      throw error;
    }
  }

  // M√©todo para an√°lise de imagem (usado pelo Telegram)
  async analyzeImage(imagePath, prompt) {
    try {
      // Para Ollama com suporte a vision, usar o modelo multimodal
      const visionModel = CONFIG.llm.imageModel || CONFIG.llm.visionModel || 'llava';
      const useApiPool = await this.shouldUseApiPool();
      let response;
      
      const requestParams = {
        model: visionModel,
        messages: [{
          role: 'user',
          content: prompt,
          images: [imagePath]
        }],
        stream: false
      };
      
      if (useApiPool) {
        logger.debug('üîÑ Usando Ollama API Pool para an√°lise de imagem');
        response = await this.ollamaApiPool.chat(requestParams);
      } else {
        logger.debug('üîÑ Usando Ollama local para an√°lise de imagem');
        response = await this.ollama.chat(requestParams);
      }
      
      return response.message.content;
    } catch (error) {
      // Fallback para local se a API falhar
      const usedApiPool = await this.shouldUseApiPool();
      if (usedApiPool) {
        try {
          logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback na an√°lise de imagem...');
          const visionModel = CONFIG.llm.imageModel || CONFIG.llm.visionModel || 'llava';
          const response = await this.ollama.chat({
            model: visionModel,
            messages: [{
              role: 'user',
              content: prompt,
              images: [imagePath]
            }]
          });
          logger.success('‚úÖ Fallback para Ollama local bem-sucedido na an√°lise de imagem');
          return response.message.content;
        } catch (fallbackError) {
          logger.error('‚ùå Fallback tamb√©m falhou na an√°lise de imagem:', fallbackError.message);
          throw new Error(`API pool failed: ${error.message}. Local fallback failed: ${fallbackError.message}`);
        }
      }
      
      logger.error('Erro ao analisar imagem:', error);
      return null;
    }
  }

  // ============ M√©todos de gerenciamento da API Ollama ============
  async getOllamaApiStatus() {
    try {
      const poolStatus = await this.ollamaApiPool.getPoolStatus();
      
      // If the pool is not enabled, return disabled status with reason
      if (!poolStatus.enabled) {
        return { 
          enabled: false, 
          mode: poolStatus.mode || 'local',
          message: 'Ollama API Pool n√£o est√° habilitado',
          totalEndpoints: 0,
          healthyEndpoints: 0,
          strategy: poolStatus.strategy,
          endpoints: []
        };
      }
      
      // If enabled but no clients, it means no endpoints are configured or enabled
      if (poolStatus.totalEndpoints === 0) {
        return {
          ...poolStatus,
          enabled: false,
          message: 'Nenhum endpoint configurado ou habilitado'
        };
      }
      
      return poolStatus;
    } catch (error) {
      logger.error('Erro ao obter status do Ollama API Pool:', error);
      return { 
        enabled: false, 
        mode: 'local',
        message: `Erro ao obter status: ${error.message}`,
        totalEndpoints: 0,
        healthyEndpoints: 0,
        strategy: 'queue_length',
        endpoints: []
      };
    }
  }

  async listModels() {
    const useApiPool = await this.shouldUseApiPool();
    
    if (useApiPool) {
      try {
        return await this.ollamaApiPool.listModels();
      } catch (error) {
        logger.warn('‚ö†Ô∏è Fallback para Ollama local na listagem de modelos');
      }
    }
    
    // Fallback para m√©todo direto do Ollama local
    return await this.ollama.list();
  }

  async pullModel(modelName) {
    const useApiPool = await this.shouldUseApiPool();
    
    if (useApiPool) {
      try {
        return await this.ollamaApiPool.pullModel(modelName);
      } catch (error) {
        logger.warn('‚ö†Ô∏è Fallback para Ollama local no pull do modelo');
      }
    }
    
    // Fallback para m√©todo direto do Ollama local
    return await this.ollama.pull({ model: modelName });
  }

  async deleteModel(modelName) {
    const useApiPool = await this.shouldUseApiPool();
    
    if (useApiPool) {
      try {
        return await this.ollamaApiPool.deleteModel(modelName);
      } catch (error) {
        logger.warn('‚ö†Ô∏è Fallback para Ollama local na dele√ß√£o do modelo');
      }
    }
    
    // Fallback para m√©todo direto do Ollama local
    return await this.ollama.delete({ model: modelName });
  }

  // M√©todo simplificado para uso direto (usado por RestAPI)
  async chatWithModel(prompt, model = CONFIG.llm.model) {
    try {
      logger.debug(`ü§ñ LLMService.chatWithModel: ${model}`);
      
      if (await this.shouldUseApiPool()) {
        logger.debug('üì° Usando OllamaAPI pool');
        try {
          const response = await this.ollamaApiPool.chat({
            model,
            messages: [{ role: 'user', content: prompt }]
          });
          return response;
        } catch (apiError) {
          logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback...', apiError.message);
        }
      }
      
      // Fallback para Ollama local
      logger.debug('üè† Usando Ollama local');
      const response = await this.ollama.chat({
        model,
        messages: [{ role: 'user', content: prompt }]
      });
      
      return response;
    } catch (error) {
      logger.error('‚ùå Erro em chatWithModel:', error);
      throw error;
    }
  }

  // Method for generating text (used by AudioTranscriber)
  async generateText(prompt, temperature = 0.7) {
    try {
      const useApiPool = await this.shouldUseApiPool();
      let response;
      
      const requestParams = {
        model: CONFIG.llm.model,
        messages: [{ role: 'user', content: prompt }],
        options: {
          temperature: temperature
        },
        stream: false
      };
      
      if (useApiPool) {
        logger.debug('üîÑ Usando Ollama API Pool para gera√ß√£o de texto');
        response = await this.ollamaApiPool.chat(requestParams);
        return response.message.content;
      } else {
        logger.debug('üîÑ Usando Ollama local para gera√ß√£o de texto');
        response = await this.ollama.chat(requestParams);
        return response.message.content;
      }
    } catch (error) {
      // Fallback para local se a API falhar
      const usedApiPool = await this.shouldUseApiPool();
      if (usedApiPool) {
        try {
          logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback na gera√ß√£o de texto...');
          const response = await this.ollama.chat({
            model: CONFIG.llm.model,
            messages: [{ role: 'user', content: prompt }],
            options: {
              temperature: temperature
            }
          });
          logger.success('‚úÖ Fallback para Ollama local bem-sucedido na gera√ß√£o de texto');
          return response.message.content;
        } catch (fallbackError) {
          logger.error('‚ùå Fallback tamb√©m falhou na gera√ß√£o de texto:', fallbackError.message);
          throw new Error(`API pool failed: ${error.message}. Local fallback failed: ${fallbackError.message}`);
        }
      }
      
      logger.error('Erro ao gerar texto LLM:', error);
      throw error;
    }
  }

  // Method for image analysis (used by WhatsAppBot)
  async generateImageAnalysis(prompt, imagePath) {
    try {
      const useApiPool = await this.shouldUseApiPool();
      let response;
      
      const visionModel = CONFIG.llm.imageModel || CONFIG.llm.visionModel || 'llava';
      
      if (useApiPool) {
        logger.debug('üîÑ Usando Ollama API Pool para an√°lise de imagem');
        response = await this.ollamaApiPool.generate({
          model: visionModel,
          prompt: prompt,
          images: [imagePath],
          stream: false
        });
        return response.response.trim();
      } else {
        logger.debug('üîÑ Usando Ollama local para an√°lise de imagem');
        response = await this.ollama.generate({
          model: visionModel,
          prompt: prompt,
          images: [imagePath],
          stream: false
        });
        return response.response.trim();
      }
    } catch (error) {
      // Fallback para local se a API falhar
      const usedApiPool = await this.shouldUseApiPool();
      if (usedApiPool) {
        try {
          logger.warn('‚ö†Ô∏è Todos os endpoints API falharam, tentando Ollama local como fallback na an√°lise de imagem...');
          const visionModel = CONFIG.llm.imageModel || CONFIG.llm.visionModel || 'llava';
          const response = await this.ollama.generate({
            model: visionModel,
            prompt: prompt,
            images: [imagePath],
            stream: false
          });
          logger.success('‚úÖ Fallback para Ollama local bem-sucedido na an√°lise de imagem');
          return response.response.trim();
        } catch (fallbackError) {
          logger.error('‚ùå Fallback tamb√©m falhou na an√°lise de imagem:', fallbackError.message);
          throw new Error(`API pool failed: ${error.message}. Local fallback failed: ${fallbackError.message}`);
        }
      }
      
      logger.error('Erro ao analisar imagem:', error);
      throw error;
    }
  }

  async onConfigurationChanged() {
    logger.info('üîÑ Configura√ß√£o alterada, reinicializando OllamaAPIPool e carregando √∫ltimo modelo...');
    await this.ollamaApiPool.reinitialize();
    await this.loadLastUsedModel();
  }

  destroy() {
    logger.info('üóëÔ∏è Destruindo LLMService...');
    this.ollamaApiPool.destroy();
  }
}

export default LLMService;
